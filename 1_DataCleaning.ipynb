{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read the `.txt` files line by line and apply these filters:\n",
    "\n",
    "1. **Remove lines containing a WhatsApp encryption notice**\n",
    "\n",
    "    - ‚ùå **Before:** `dd/mm/yyyy, hh:mm - Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more.`\n",
    "    - ‚úÖ **After:** (Removed)\n",
    "\n",
    "2. **Remove lines with `<Media omitted>`**\n",
    "\n",
    "    - ‚ùå **Before:** `dd/mm/yyyy, hh:mm - Person: <Media omitted>`\n",
    "    - ‚úÖ **After:** (Removed)\n",
    "\n",
    "3. **Remove lines containing email addresses**\n",
    "\n",
    "    - ‚ùå **Before:** `dd/mm/yyyy, hh:mm - Person: example@gmail.com`\n",
    "    - ‚úÖ **After:** (Removed)\n",
    "\n",
    "4. **Remove lines containing links**\n",
    "\n",
    "    - ‚ùå **Before:** `dd/mm/yyyy, hh:mm - Person: https://www.example.com/`\n",
    "    - ‚úÖ **After:** (Removed)\n",
    "\n",
    "5. **Replace `<This message was edited>` with an empty string**\n",
    "\n",
    "    - ‚ùå **Before:** `dd/mm/yyyy, hh:mm - Person: hey, how are you? <This message was edited>`\n",
    "    - ‚úÖ **After:** `dd/mm/yyyy, hh:mm - Person: hey, how are you?`\n",
    "\n",
    "6. **Remove lines with the text `You deleted this message`**\n",
    "\n",
    "    - ‚ùå **Before:** `dd/mm/yyyy, hh:mm - Person: You deleted this message`\n",
    "    - ‚úÖ **After:** (Removed)\n",
    "\n",
    "7. **Remove lines with the text `null`**\n",
    "\n",
    "    - ‚ùå **Before:** `dd/mm/yyyy, hh:mm - Person: null`\n",
    "    - ‚úÖ **After:** (Removed)\n",
    "\n",
    "8. **Remove lines with the text `created group`**\n",
    "\n",
    "    - ‚ùå **Before:** `dd/mm/yyyy, hh:mm - Person created group \"group name\"`\n",
    "    - ‚úÖ **After:** (Removed)\n",
    "\n",
    "9. **Remove lines with the text `added you`**\n",
    "\n",
    "    - ‚ùå **Before:** `dd/mm/yyyy, hh:mm - Person added you`\n",
    "    - ‚úÖ **After:** (Removed)\n",
    "\n",
    "10. **Replace tagging (`@person`) with an empty string**\n",
    "\n",
    "-   ‚ùå **Before:** `dd/mm/yyyy, hh:mm - Person: @person are you coming?`\n",
    "-   ‚úÖ **After:** `dd/mm/yyyy, hh:mm - Person: are you coming?`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_whatsapp_chat(file_path: str) -> pd.DataFrame:\n",
    "    # Define filtering patterns\n",
    "    encryption_message = \"Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more.\"\n",
    "    media_pattern = \"<Media omitted>\"\n",
    "    email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}'\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    edited_message = \"<This message was edited>\"\n",
    "    deleted_message = \"You deleted this message\"\n",
    "    null_message = \"null\"\n",
    "    created_group_message = \"created group\"\n",
    "    added_you_to_group_message = \"added you\"\n",
    "    tagging_pattern = r'@[\\w]+'\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Apply filters to remove unwanted lines\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if (\n",
    "            encryption_message not in line and\n",
    "            deleted_message not in line and\n",
    "            null_message != line.split(\" \")[-1] and\n",
    "            media_pattern not in line and\n",
    "            created_group_message not in line and\n",
    "            added_you_to_group_message not in line and\n",
    "            not re.search(email_pattern, line) and\n",
    "            not re.search(url_pattern, line)\n",
    "        ):\n",
    "            line = line.replace(edited_message, \"\").strip()\n",
    "            line = re.sub(tagging_pattern, \"\", line).strip()\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Regular expression to match WhatsApp message format\n",
    "    pattern = r'(\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2}) - (.*?): (.*?)(?=\\n\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2} -|$)'\n",
    "    content = '\\n'.join(filtered_lines)\n",
    "    messages = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "    df = pd.DataFrame(messages, columns=['timestamp', 'sender', 'message'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%m/%Y, %H:%M')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `all_chats` dictionary holds the content of each file as a dataframe with three columns: `timestamp`, `sender`, and `message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "all_chats = {}\n",
    "data_directory = Path(\"WhatsApp\")\n",
    "for file in data_directory.glob('*.txt'):\n",
    "    file_name = file.stem\n",
    "    all_chats[file_name] = read_whatsapp_chat(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text should be merged into a single sequence to prepare it for the next step, where the BPE algorithm will be applied and the text will be encoded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_sequence = \"\"\n",
    "for file_name in all_chats.keys():\n",
    "    text_sequence += \" \".join(all_chats[file_name]['message'].values)\n",
    "\n",
    "len(text_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/combined_text.txt\", \"w\") as f:\n",
    "    f.write(text_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined text saved to output/combined_text.txt\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def read_whatsapp_chat(file_path: str) -> pd.DataFrame:\n",
    "    with open(str(file_path), 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Remove system messages (media, encryption, deleted messages)\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if not any(exclude in line for exclude in [\n",
    "            \"Messages and calls are end-to-end encrypted\",\n",
    "            \"<Media omitted>\",\n",
    "            \"This message was deleted\"\n",
    "        ]):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Join messages as a single text\n",
    "    content = '\\n'.join(filtered_lines)\n",
    "\n",
    "    # Updated regex pattern to correctly extract messages\n",
    "    pattern = r'\\[(\\d{2}/\\d{2}/\\d{2,4}), (\\d{1,2}:\\d{2}:\\d{2}\\s?[APM\\u202F]*)\\] (.*?): (.*?)(?=\\n\\[\\d{2}/\\d{2}/\\d{2,4}, \\d{1,2}:\\d{2}:\\d{2}\\s?[APM\\u202F]*\\]|\\Z)'\n",
    "\n",
    "    messages = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(messages, columns=['date', 'time', 'sender', 'message'])\n",
    "    df['timestamp'] = pd.to_datetime(df['date'] + \" \" + df['time'], format='%d/%m/%y %I:%M:%S %p')\n",
    "    df.drop(columns=['date', 'time'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Read all WhatsApp chats\n",
    "all_chats = {}\n",
    "data_directory = Path(\"WhatsApp\")\n",
    "\n",
    "if not data_directory.exists():\n",
    "    print(\"Error: WhatsApp directory does not exist!\")\n",
    "elif not list(data_directory.glob(\"*.txt\")):\n",
    "    print(\"Error: No .txt files found in WhatsApp directory!\")\n",
    "else:\n",
    "    for file in data_directory.glob('*.txt'):\n",
    "        file_name = file.stem\n",
    "        all_chats[file_name] = read_whatsapp_chat(file)\n",
    "\n",
    "# Merge all messages\n",
    "text_sequence = \" \".join(\n",
    "    msg for chat in all_chats.values() for msg in chat['message'].values\n",
    ")\n",
    "\n",
    "# Save output\n",
    "output_directory = Path(\"./output\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_directory / \"combined_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text_sequence)\n",
    "\n",
    "print(f\"Combined text saved to {output_directory}/combined_text.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No messages extracted from WhatsApp/test.txt\n",
      "Warning: No data extracted from test\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21338/169822204.py:39: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  df['timestamp'] = pd.to_datetime(df['date'] + \" \" + df['time'], errors='coerce')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No messages extracted from WhatsApp/2.txt\n",
      "Warning: No data extracted from 2\n",
      "Warning: No messages extracted from WhatsApp/4.txt\n",
      "Warning: No data extracted from 4\n",
      "‚úÖ Ollama training data saved to output/ollama_training.json\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# List of names that should be considered as \"assistant\"\n",
    "ASSISTANT_NAMES = [\"Aditya Pandey\", \"Umyal Dixit\"]\n",
    "\n",
    "def read_whatsapp_chat(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Reads a WhatsApp chat file and converts it into a structured DataFrame.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Remove system messages (e.g., media omitted, encryption notices)\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if not any(exclude in line for exclude in [\n",
    "            \"Messages and calls are end-to-end encrypted\",\n",
    "            \"<Media omitted>\",\n",
    "            \"This message was deleted\",\n",
    "            \"<This message was edited>\"\n",
    "        ]):\n",
    "            filtered_lines.append(line.strip())\n",
    "\n",
    "    # Combine messages into a single string\n",
    "    content = '\\n'.join(filtered_lines)\n",
    "\n",
    "    # Updated regex pattern to handle different timestamp formats\n",
    "    pattern = r'\\[(\\d{1,2}/\\d{1,2}/\\d{2,4}), (\\d{1,2}:\\d{2}:\\d{2}[\\s\\u202F]?(?:AM|PM)?)\\] (.*?): (.*?)(?=\\n\\[\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2}:\\d{2}[\\s\\u202F]?(?:AM|PM)?\\]|\\Z)'\n",
    "\n",
    "    messages = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "    # Debugging: Print extracted messages\n",
    "    if not messages:\n",
    "        print(f\"Warning: No messages extracted from {file_path}\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(messages, columns=['date', 'time', 'sender', 'message'])\n",
    "    df['timestamp'] = pd.to_datetime(df['date'] + \" \" + df['time'], errors='coerce')\n",
    "    df.drop(columns=['date', 'time'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Read all WhatsApp chat files\n",
    "data_directory = Path(\"WhatsApp\")\n",
    "all_chats = {}\n",
    "\n",
    "if not data_directory.exists():\n",
    "    print(\"Error: WhatsApp directory does not exist!\")\n",
    "elif not list(data_directory.glob(\"*.txt\")):\n",
    "    print(\"Error: No .txt files found in WhatsApp directory!\")\n",
    "else:\n",
    "    for file in data_directory.glob('*.txt'):\n",
    "        file_name = file.stem\n",
    "        df = read_whatsapp_chat(file)\n",
    "        if not df.empty:\n",
    "            all_chats[file_name] = df\n",
    "        else:\n",
    "            print(f\"Warning: No data extracted from {file_name}\")\n",
    "\n",
    "# Convert to Ollama training JSON format\n",
    "ollama_data = []\n",
    "\n",
    "for file_name, df in all_chats.items():\n",
    "    messages = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        role = \"assistant\" if row[\"sender\"] in ASSISTANT_NAMES else \"user\"\n",
    "        messages.append({\"role\": role, \"content\": row[\"message\"]})\n",
    "\n",
    "    if messages:\n",
    "        ollama_data.append({\n",
    "            \"system\": \"You are a helpful AI trained on WhatsApp conversations.\",\n",
    "            \"messages\": messages\n",
    "        })\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_directory = Path(\"./output\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save JSON file\n",
    "output_file = output_directory / \"ollama_training.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ollama_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ Ollama training data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Processing test.txt...\n",
      "üîÑ Processing 2.txt...\n",
      "üîÑ Processing ad.txt...\n",
      "üîÑ Processing 4.txt...\n",
      "‚úÖ Training data saved to output/ollama_training.jsonl\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Folder containing WhatsApp chat files\n",
    "input_folder = \"WhatsApp/\"\n",
    "output_folder = \"output/\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define assistant user\n",
    "assistant_user = \"Umyal Dixit\"\n",
    "\n",
    "# WhatsApp chat regex pattern\n",
    "pattern = r\"(\\d{2}/\\d{2}/\\d{2}), (\\d{1,2}:\\d{2}‚ÄØ?[ap]m) - (.*?): (.*)\"\n",
    "\n",
    "# Store all messages\n",
    "all_data = []\n",
    "\n",
    "# Process all .txt files in the folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        print(f\"üîÑ Processing {filename}...\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            match = re.match(pattern, line)\n",
    "            if match:\n",
    "                date, time, user, message = match.groups()\n",
    "                role = \"assistant\" if user == assistant_user else \"user\"\n",
    "                all_data.append({\"timestamp\": f\"{date} {time}\", \"user\": user, \"message\": message, \"role\": role})\n",
    "\n",
    "# Convert to JSONL format\n",
    "jsonl_output_file = os.path.join(output_folder, \"ollama_training.jsonl\")\n",
    "\n",
    "with open(jsonl_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in all_data:\n",
    "        json.dump(entry, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"‚úÖ Training data saved to {jsonl_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8355/8355 [00:00<00:00, 45240.36 examples/s]\n",
      "Creating json from Arrow format: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9/9 [00:00<00:00, 489.73ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ WhatsApp chat data formatted and saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Define folder containing WhatsApp chat files\n",
    "whatsapp_folder = \"WhatsApp\"\n",
    "\n",
    "# WhatsApp message regex pattern (handles both AM/PM and 24-hour formats)\n",
    "whatsapp_pattern = r\"(\\d{2}/\\d{2}/\\d{2,4}), (\\d{1,2}:\\d{2}\\s?(?:AM|PM|am|pm)?) - ([^:]+): (.+)\"\n",
    "\n",
    "# Store extracted data\n",
    "data = []\n",
    "\n",
    "# Loop through all .txt files in the folder\n",
    "for filename in os.listdir(whatsapp_folder):\n",
    "    file_path = os.path.join(whatsapp_folder, filename)\n",
    "\n",
    "    if filename.endswith(\".txt\"):  # Process only text files\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Store the last user message to match with the assistant's reply\n",
    "        last_user_message = None\n",
    "        last_user_name = None\n",
    "\n",
    "        for line in lines:\n",
    "            match = re.match(whatsapp_pattern, line)\n",
    "            if match:\n",
    "                date, time, sender, message = match.groups()\n",
    "\n",
    "                # Skip system messages\n",
    "                if \"Messages and calls are end-to-end encrypted\" in message:\n",
    "                    continue\n",
    "\n",
    "                # Assign roles\n",
    "                role = \"assistant\" if sender.strip() == \"Umyal Dixit\" else \"user\"\n",
    "\n",
    "                if role == \"user\":\n",
    "                    last_user_message = message.strip()\n",
    "                    last_user_name = sender.strip()\n",
    "                elif role == \"assistant\" and last_user_message:\n",
    "                    # Create Question-Answer pair\n",
    "                    data.append({\n",
    "                        \"question\": f\"{last_user_name}: {last_user_message}\",\n",
    "                        \"answer\": f\"Umyal Dixit: {message.strip()}\"\n",
    "                    })\n",
    "                    last_user_message = None  # Reset for next pair\n",
    "\n",
    "# Convert extracted data to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset format\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Format for training\n",
    "dataset = dataset.map(lambda x: {\"text\": f\"### Question: {x['question']}\\n### Answer: {x['answer']}\"})\n",
    "\n",
    "# Save formatted dataset\n",
    "df.to_csv(\"whatsapp_dataset.csv\", index=False)\n",
    "dataset.to_json(\"whatsapp_dataset.json\", orient=\"records\")\n",
    "\n",
    "print(\"‚úÖ WhatsApp chat data formatted and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "================================================================================\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('0'), PosixPath('1')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('@/tmp/.ICE-unix/2009,unix/pop-os'), PosixPath('local/pop-os')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('vs/workbench/api/node/extensionHostProcess')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('{}}'), PosixPath('\"en\",\"defaultMessagesFile\"'), PosixPath('\"en-us\",\"osLocale\"'), PosixPath('{\"userLocale\"'), PosixPath('\"en-us\",\"availableLanguages\"'), PosixPath('\"/usr/share/code/resources/app/out/nls.messages.json\",\"locale\"'), PosixPath('\"en-us\",\"resolvedLanguage\"')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/etc/xdg/xdg-pop')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda/lib64')}\n",
      "DEBUG: Possible options found for libcudart.so: set()\n",
      "CUDA SETUP: PyTorch settings found: CUDA_VERSION=124, Highest Compute Capability: 8.6.\n",
      "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "CUDA SETUP: Required library version not found: libbitsandbytes_cuda124.so. Maybe you need to compile it from source?\n",
      "CUDA SETUP: Defaulting to libbitsandbytes_cpu.so...\n",
      "\n",
      "================================================ERROR=====================================\n",
      "CUDA SETUP: CUDA detection failed! Possible reasons:\n",
      "1. You need to manually override the PyTorch CUDA version. Please see: \"https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "2. CUDA driver not installed\n",
      "3. CUDA not installed\n",
      "4. You have multiple conflicting CUDA libraries\n",
      "5. Required library not pre-compiled for this bitsandbytes release!\n",
      "CUDA SETUP: If you compiled from source, try again with `make CUDA_VERSION=DETECTED_CUDA_VERSION` for example, `make CUDA_VERSION=113`.\n",
      "CUDA SETUP: The CUDA version for the compile might depend on your conda install. Inspect CUDA version via `conda list | grep cuda`.\n",
      "================================================================================\n",
      "\n",
      "CUDA SETUP: Problem: The main issue seems to be that the main CUDA runtime library was not detected.\n",
      "CUDA SETUP: Solution 1: To solve the issue the libcudart.so location needs to be added to the LD_LIBRARY_PATH variable\n",
      "CUDA SETUP: Solution 1a): Find the cuda runtime library via: find / -name libcudart.so 2>/dev/null\n",
      "CUDA SETUP: Solution 1b): Once the library is found add it to the LD_LIBRARY_PATH: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:FOUND_PATH_FROM_1a\n",
      "CUDA SETUP: Solution 1c): For a permanent solution add the export from 1b into your .bashrc file, located at ~/.bashrc\n",
      "CUDA SETUP: Solution 2: If no library was found in step 1a) you need to install CUDA.\n",
      "CUDA SETUP: Solution 2a): Download CUDA install script: wget https://github.com/TimDettmers/bitsandbytes/blob/main/cuda_install.sh\n",
      "CUDA SETUP: Solution 2b): Install desired CUDA version to desired location. The syntax is bash cuda_install.sh CUDA_VERSION PATH_TO_INSTALL_INTO.\n",
      "CUDA SETUP: Solution 2b): For example, \"bash cuda_install.sh 113 ~/local/\" will download CUDA 11.3 and install into the folder ~/local\n",
      "CUDA SETUP: Setup Failed!\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.training_args because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/transformers/utils/import_utils.py:1099\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/transformers/training_args.py:68\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AcceleratorState, PartialState\n\u001b[0;32m---> 68\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01maccelerate\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DistributedType\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/accelerate/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0.21.0\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maccelerator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Accelerator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbig_modeling\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      5\u001b[0m     cpu_offload,\n\u001b[1;32m      6\u001b[0m     cpu_offload_with_hook,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m     load_checkpoint_and_dispatch,\n\u001b[1;32m     12\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/accelerate/accelerator.py:35\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhooks\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhooks\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpointing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_accelerator_state, load_custom_state, save_accelerator_state, save_custom_state\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoaderDispatcher, prepare_data_loader, skip_first_batches\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/accelerate/checkpointing.py:24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcuda\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mamp\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GradScaler\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     25\u001b[0m     MODEL_NAME,\n\u001b[1;32m     26\u001b[0m     OPTIMIZER_NAME,\n\u001b[1;32m     27\u001b[0m     RNG_STATE_NAME,\n\u001b[1;32m     28\u001b[0m     SCALER_NAME,\n\u001b[1;32m     29\u001b[0m     SCHEDULER_NAME,\n\u001b[1;32m     30\u001b[0m     get_pretty_name,\n\u001b[1;32m     31\u001b[0m     is_tpu_available,\n\u001b[1;32m     32\u001b[0m     is_xpu_available,\n\u001b[1;32m     33\u001b[0m     save,\n\u001b[1;32m     34\u001b[0m )\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/accelerate/utils/__init__.py:131\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeepspeed\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    123\u001b[0m         DeepSpeedEngineWrapper,\n\u001b[1;32m    124\u001b[0m         DeepSpeedOptimizerWrapper,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    128\u001b[0m         HfDeepSpeedConfig,\n\u001b[1;32m    129\u001b[0m     )\n\u001b[0;32m--> 131\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbnb\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m has_4bit_bnb_layers, load_and_quantize_model\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_fsdp_model, load_fsdp_optimizer, save_fsdp_model, save_fsdp_optimizer\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/accelerate/utils/bnb.py:42\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_bnb_available():\n\u001b[0;32m---> 42\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcopy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deepcopy\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/bitsandbytes/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cuda_setup, utils, research\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      8\u001b[0m     MatmulLtState,\n\u001b[1;32m      9\u001b[0m     bmm_cublas,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m     matmul_4bit\n\u001b[1;32m     14\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/bitsandbytes/research/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mautograd\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_functions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     switchback_bnb,\n\u001b[1;32m      4\u001b[0m     matmul_fp8_global,\n\u001b[1;32m      5\u001b[0m     matmul_fp8_mixed,\n\u001b[1;32m      6\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/bitsandbytes/research/nn/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LinearFP8Mixed, LinearFP8Global\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/bitsandbytes/research/nn/modules.py:8\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbnb\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GlobalOptimManager\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OutlierTracer, find_outlier_dims\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/bitsandbytes/optim/__init__.py:6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) Facebook, Inc. and its affiliates.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# This source code is licensed under the MIT license found in the\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# LICENSE file in the root directory of this source tree.\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mbitsandbytes\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcextension\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m COMPILED_WITH_CUDA\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01madagrad\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Adagrad, Adagrad8bit, Adagrad32bit\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/bitsandbytes/cextension.py:20\u001b[0m\n\u001b[1;32m     19\u001b[0m     CUDASetup\u001b[38;5;241m.\u001b[39mget_instance()\u001b[38;5;241m.\u001b[39mprint_log_stack()\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'''\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124m    CUDA Setup failed despite GPU being available. Please run the following command to get more information:\u001b[39m\n\u001b[1;32m     22\u001b[0m \n\u001b[1;32m     23\u001b[0m \u001b[38;5;124m    python -m bitsandbytes\u001b[39m\n\u001b[1;32m     24\u001b[0m \n\u001b[1;32m     25\u001b[0m \u001b[38;5;124m    Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124m    to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124m    and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\u001b[39m\u001b[38;5;124m'''\u001b[39m)\n\u001b[1;32m     28\u001b[0m lib\u001b[38;5;241m.\u001b[39mcadam32bit_grad_fp32 \u001b[38;5;66;03m# runs on an error if the library could not be found -> COMPILED_WITH_CUDA=False\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: \n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpeft\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, PeftModel\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtrl\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SFTTrainer\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1075\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/transformers/utils/import_utils.py:1089\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1087\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[1;32m   1088\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m-> 1089\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1090\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/sukkun_models/lib/python3.10/site-packages/transformers/utils/import_utils.py:1101\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1099\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m-> 1101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1102\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1103\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1104\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.training_args because of the following error (look up to see its traceback):\n\n        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n\n        python -m bitsandbytes\n\n        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Step 1: Load and Format WhatsApp Dataset\n",
    "df = pd.read_csv('whatsapp_dataset.csv')  # Ensure the dataset is in CSV format\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(lambda x: {\"text\": f\"### Question: {x['question']}\\n### Answer: {x['answer']}\"})\n",
    "\n",
    "# Step 2: Initialize Dolphin 3 Model & Tokenizer\n",
    "model_name = \"dolphin3\"  # Replace with Dolphin-3 when available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# LoRA Configuration for Efficient Fine-Tuning\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Load Base Model with Quantization (for lower VRAM usage)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")\n",
    "\n",
    "# Step 3: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dolphin3_finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"  # Change to \"wandb\" if using Weights & Biases\n",
    ")\n",
    "\n",
    "# Step 4: Fine-Tune the Model\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Step 5: Save Fine-Tuned Model Locally\n",
    "trainer.model.save_pretrained('finetuned_dolphin3')\n",
    "\n",
    "# Step 6: Evaluate & Inference\n",
    "pipe = pipeline('text-generation', model=trainer.model, tokenizer=tokenizer)\n",
    "output = pipe(\"### Question: What is your name?\")[0]['generated_text']\n",
    "print(output)\n",
    "\n",
    "# Step 7: Free Up Memory\n",
    "del base_model, trainer, pipe\n",
    "gc.collect()\n",
    "\n",
    "# Step 8: Merge LoRA with Base Model & Upload to Hugging Face Hub\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(base_model, 'finetuned_dolphin3')\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Step 9: Push Fine-Tuned Model to Hugging Face Hub\n",
    "# model.push_to_hub(\"your_huggingface_username/finetuned_dolphin3\")\n",
    "# tokenizer.push_to_hub(\"your_huggingface_username/finetuned_dolphin3\")\n",
    "\n",
    "print(\"‚úÖ Fine-tuned Dolphin 3 model saved & uploaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sukkun_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
