{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading files\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will read the `.txt` files line by line and apply these filters:\n",
    "\n",
    "1. **Remove lines containing a WhatsApp encryption notice**\n",
    "\n",
    "    - ❌ **Before:** `dd/mm/yyyy, hh:mm - Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more.`\n",
    "    - ✅ **After:** (Removed)\n",
    "\n",
    "2. **Remove lines with `<Media omitted>`**\n",
    "\n",
    "    - ❌ **Before:** `dd/mm/yyyy, hh:mm - Person: <Media omitted>`\n",
    "    - ✅ **After:** (Removed)\n",
    "\n",
    "3. **Remove lines containing email addresses**\n",
    "\n",
    "    - ❌ **Before:** `dd/mm/yyyy, hh:mm - Person: example@gmail.com`\n",
    "    - ✅ **After:** (Removed)\n",
    "\n",
    "4. **Remove lines containing links**\n",
    "\n",
    "    - ❌ **Before:** `dd/mm/yyyy, hh:mm - Person: https://www.example.com/`\n",
    "    - ✅ **After:** (Removed)\n",
    "\n",
    "5. **Replace `<This message was edited>` with an empty string**\n",
    "\n",
    "    - ❌ **Before:** `dd/mm/yyyy, hh:mm - Person: hey, how are you? <This message was edited>`\n",
    "    - ✅ **After:** `dd/mm/yyyy, hh:mm - Person: hey, how are you?`\n",
    "\n",
    "6. **Remove lines with the text `You deleted this message`**\n",
    "\n",
    "    - ❌ **Before:** `dd/mm/yyyy, hh:mm - Person: You deleted this message`\n",
    "    - ✅ **After:** (Removed)\n",
    "\n",
    "7. **Remove lines with the text `null`**\n",
    "\n",
    "    - ❌ **Before:** `dd/mm/yyyy, hh:mm - Person: null`\n",
    "    - ✅ **After:** (Removed)\n",
    "\n",
    "8. **Remove lines with the text `created group`**\n",
    "\n",
    "    - ❌ **Before:** `dd/mm/yyyy, hh:mm - Person created group \"group name\"`\n",
    "    - ✅ **After:** (Removed)\n",
    "\n",
    "9. **Remove lines with the text `added you`**\n",
    "\n",
    "    - ❌ **Before:** `dd/mm/yyyy, hh:mm - Person added you`\n",
    "    - ✅ **After:** (Removed)\n",
    "\n",
    "10. **Replace tagging (`@person`) with an empty string**\n",
    "\n",
    "-   ❌ **Before:** `dd/mm/yyyy, hh:mm - Person: @person are you coming?`\n",
    "-   ✅ **After:** `dd/mm/yyyy, hh:mm - Person: are you coming?`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def read_whatsapp_chat(file_path: str) -> pd.DataFrame:\n",
    "    # Define filtering patterns\n",
    "    encryption_message = \"Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more.\"\n",
    "    media_pattern = \"<Media omitted>\"\n",
    "    email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}'\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    edited_message = \"<This message was edited>\"\n",
    "    deleted_message = \"You deleted this message\"\n",
    "    null_message = \"null\"\n",
    "    created_group_message = \"created group\"\n",
    "    added_you_to_group_message = \"added you\"\n",
    "    tagging_pattern = r'@[\\w]+'\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Apply filters to remove unwanted lines\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if (\n",
    "            encryption_message not in line and\n",
    "            deleted_message not in line and\n",
    "            null_message != line.split(\" \")[-1] and\n",
    "            media_pattern not in line and\n",
    "            created_group_message not in line and\n",
    "            added_you_to_group_message not in line and\n",
    "            not re.search(email_pattern, line) and\n",
    "            not re.search(url_pattern, line)\n",
    "        ):\n",
    "            line = line.replace(edited_message, \"\").strip()\n",
    "            line = re.sub(tagging_pattern, \"\", line).strip()\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Regular expression to match WhatsApp message format\n",
    "    pattern = r'(\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2}) - (.*?): (.*?)(?=\\n\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2} -|$)'\n",
    "    content = '\\n'.join(filtered_lines)\n",
    "    messages = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "    df = pd.DataFrame(messages, columns=['timestamp', 'sender', 'message'])\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%m/%Y, %H:%M')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def read_whatsapp_chat(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read and process a WhatsApp chat text file, filtering out unwanted messages.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the WhatsApp chat text file\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Processed DataFrame with filtered messages\n",
    "    \"\"\"\n",
    "    # Define filtering patterns\n",
    "    encryption_message = \"Messages and calls are end-to-end encrypted. No one outside of this chat, not even WhatsApp, can read or listen to them. Tap to learn more.\"\n",
    "    media_pattern = \"<Media omitted>\"\n",
    "    email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}'\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    edited_message = \"<This message was edited>\"\n",
    "    deleted_message = \"You deleted this message\"\n",
    "    null_message = \"null\"\n",
    "    created_group_message = \"created group\"\n",
    "    added_you_to_group_message = \"added you\"\n",
    "    tagging_pattern = r'@[\\w]+'\n",
    "\n",
    "    # Read the file\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Apply filters to remove unwanted lines\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if (\n",
    "            encryption_message not in line and\n",
    "            deleted_message not in line and\n",
    "            null_message != line.strip().split(\" \")[-1] and\n",
    "            media_pattern not in line and\n",
    "            created_group_message not in line and\n",
    "            added_you_to_group_message not in line and\n",
    "            not re.search(email_pattern, line) and\n",
    "            not re.search(url_pattern, line)\n",
    "        ):\n",
    "            line = line.replace(edited_message, \"\").strip()\n",
    "            line = re.sub(tagging_pattern, \"\", line).strip()\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Regular expression to match WhatsApp message format\n",
    "    pattern = r'(\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2}) - (.*?): (.*?)(?=\\n\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2} -|$)'\n",
    "    content = '\\n'.join(filtered_lines)\n",
    "    messages = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "    # Create DataFrame\n",
    "    if messages:\n",
    "        df = pd.DataFrame(messages, columns=['timestamp', 'sender', 'message'])\n",
    "        df['timestamp'] = pd.to_datetime(df['timestamp'], format='%d/%m/%Y, %H:%M')\n",
    "\n",
    "        # Remove empty messages\n",
    "        df = df[df['message'].str.strip() != '']\n",
    "\n",
    "        return df\n",
    "\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def process_chat_files(folder_path: str, answer_sender: str) -> list:\n",
    "    \"\"\"\n",
    "    Process multiple WhatsApp chat files and extract Q&A pairs.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing chat text files\n",
    "        answer_sender (str): Name of the sender who provides answers\n",
    "\n",
    "    Returns:\n",
    "        list: List of dictionaries containing questions and answers\n",
    "    \"\"\"\n",
    "    # List all text files in the folder\n",
    "    chat_files = [f for f in os.listdir(folder_path) if f.endswith('.txt')]\n",
    "\n",
    "    # Collect all processed DataFrames\n",
    "    all_chats = []\n",
    "\n",
    "    for file in chat_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        chat_df = read_whatsapp_chat(file_path)\n",
    "\n",
    "        if not chat_df.empty:\n",
    "            all_chats.append(chat_df)\n",
    "\n",
    "    # Combine all chat DataFrames\n",
    "    if all_chats:\n",
    "        combined_df = pd.concat(all_chats, ignore_index=True)\n",
    "\n",
    "        # Sort by timestamp\n",
    "        combined_df = combined_df.sort_values('timestamp')\n",
    "\n",
    "        # Separate potential questions (all non-answer senders) and answers\n",
    "        answers_df = combined_df[combined_df['sender'] == answer_sender]\n",
    "\n",
    "        # Prepare result list\n",
    "        qa_pairs = []\n",
    "\n",
    "        # Find Q&A pairs\n",
    "        for answer in answers_df.itertuples():\n",
    "            # Find the most recent message from a different sender before this answer\n",
    "            question_candidates = combined_df[\n",
    "                (combined_df['timestamp'] < answer.timestamp) &\n",
    "                (combined_df['sender'] != answer_sender)\n",
    "            ]\n",
    "\n",
    "            if not question_candidates.empty:\n",
    "                # Take the most recent question\n",
    "                question = question_candidates.iloc[-1]\n",
    "\n",
    "                # Only add if both question and answer are non-empty\n",
    "                if (str(question.message).strip() and\n",
    "                    str(answer.message).strip()):\n",
    "                    qa_pairs.append({\n",
    "                        \"question\": question.message,\n",
    "                        \"question_sender\": question.sender,\n",
    "                        \"answer\": answer.message,\n",
    "                        \"timestamp\": answer.timestamp.strftime('%Y-%m-%d %H:%M:%S')\n",
    "                    })\n",
    "\n",
    "        return qa_pairs\n",
    "\n",
    "    return []\n",
    "\n",
    "def main(folder_path: str, answer_sender: str, output_file: str = 'qa_pairs.json'):\n",
    "    \"\"\"\n",
    "    Main function to process chat files and save results to JSON.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing chat text files\n",
    "        answer_sender (str): Name of the sender who provides answers\n",
    "        output_file (str, optional): Path to save the output JSON file\n",
    "    \"\"\"\n",
    "    # Process chat files\n",
    "    qa_pairs = process_chat_files(folder_path, answer_sender)\n",
    "\n",
    "    # Save to JSON\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"Processed {len(qa_pairs)} Q&A pairs. Saved to {output_file}\")\n",
    "    print(\"First few pairs:\")\n",
    "    print(json.dumps(qa_pairs[:3], ensure_ascii=False, indent=2))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = './WhatsApp/'\n",
    "    answer_sender = 'Umyal Dixit'\n",
    "\n",
    "    # Process and save to JSON\n",
    "    main(folder_path, answer_sender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `all_chats` dictionary holds the content of each file as a dataframe with three columns: `timestamp`, `sender`, and `message`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "all_chats = {}\n",
    "data_directory = Path(\"WhatsApp\")\n",
    "for file in data_directory.glob('*.txt'):\n",
    "    file_name = file.stem\n",
    "    all_chats[file_name] = read_whatsapp_chat(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text should be merged into a single sequence to prepare it for the next step, where the BPE algorithm will be applied and the text will be encoded.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_sequence = \"\"\n",
    "for file_name in all_chats.keys():\n",
    "    text_sequence += \" \".join(all_chats[file_name]['message'].values)\n",
    "\n",
    "len(text_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./output/combined_text.txt\", \"w\") as f:\n",
    "    f.write(text_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def read_whatsapp_chat(file_path: str) -> pd.DataFrame:\n",
    "    with open(str(file_path), 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Remove system messages (media, encryption, deleted messages)\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if not any(exclude in line for exclude in [\n",
    "            \"Messages and calls are end-to-end encrypted\",\n",
    "            \"<Media omitted>\",\n",
    "            \"This message was deleted\"\n",
    "        ]):\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Join messages as a single text\n",
    "    content = '\\n'.join(filtered_lines)\n",
    "\n",
    "    # Updated regex pattern to correctly extract messages\n",
    "    pattern = r'\\[(\\d{2}/\\d{2}/\\d{2,4}), (\\d{1,2}:\\d{2}:\\d{2}\\s?[APM\\u202F]*)\\] (.*?): (.*?)(?=\\n\\[\\d{2}/\\d{2}/\\d{2,4}, \\d{1,2}:\\d{2}:\\d{2}\\s?[APM\\u202F]*\\]|\\Z)'\n",
    "\n",
    "    messages = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(messages, columns=['date', 'time', 'sender', 'message'])\n",
    "    df['timestamp'] = pd.to_datetime(df['date'] + \" \" + df['time'], format='%d/%m/%y %I:%M:%S %p')\n",
    "    df.drop(columns=['date', 'time'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Read all WhatsApp chats\n",
    "all_chats = {}\n",
    "data_directory = Path(\"WhatsApp\")\n",
    "\n",
    "if not data_directory.exists():\n",
    "    print(\"Error: WhatsApp directory does not exist!\")\n",
    "elif not list(data_directory.glob(\"*.txt\")):\n",
    "    print(\"Error: No .txt files found in WhatsApp directory!\")\n",
    "else:\n",
    "    for file in data_directory.glob('*.txt'):\n",
    "        file_name = file.stem\n",
    "        all_chats[file_name] = read_whatsapp_chat(file)\n",
    "\n",
    "# Merge all messages\n",
    "text_sequence = \" \".join(\n",
    "    msg for chat in all_chats.values() for msg in chat['message'].values\n",
    ")\n",
    "\n",
    "# Save output\n",
    "output_directory = Path(\"./\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(output_directory / \"combined_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(text_sequence)\n",
    "\n",
    "print(f\"Combined text saved to {output_directory}/combined_text.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# List of names that should be considered as \"assistant\"\n",
    "ASSISTANT_NAMES = [\"Aditya Pandey\", \"Umyal Dixit\"]\n",
    "\n",
    "def read_whatsapp_chat(file_path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Reads a WhatsApp chat file and converts it into a structured DataFrame.\"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Remove system messages (e.g., media omitted, encryption notices)\n",
    "    filtered_lines = []\n",
    "    for line in lines:\n",
    "        if not any(exclude in line for exclude in [\n",
    "            \"Messages and calls are end-to-end encrypted\",\n",
    "            \"<Media omitted>\",\n",
    "            \"This message was deleted\",\n",
    "            \"<This message was edited>\"\n",
    "        ]):\n",
    "            filtered_lines.append(line.strip())\n",
    "\n",
    "    # Combine messages into a single string\n",
    "    content = '\\n'.join(filtered_lines)\n",
    "\n",
    "    # Updated regex pattern to handle different timestamp formats\n",
    "    pattern = r'\\[(\\d{1,2}/\\d{1,2}/\\d{2,4}), (\\d{1,2}:\\d{2}:\\d{2}[\\s\\u202F]?(?:AM|PM)?)\\] (.*?): (.*?)(?=\\n\\[\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2}:\\d{2}[\\s\\u202F]?(?:AM|PM)?\\]|\\Z)'\n",
    "\n",
    "    messages = re.findall(pattern, content, re.DOTALL)\n",
    "\n",
    "    # Debugging: Print extracted messages\n",
    "    if not messages:\n",
    "        print(f\"Warning: No messages extracted from {file_path}\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(messages, columns=['date', 'time', 'sender', 'message'])\n",
    "    df['timestamp'] = pd.to_datetime(df['date'] + \" \" + df['time'], errors='coerce')\n",
    "    df.drop(columns=['date', 'time'], inplace=True)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Read all WhatsApp chat files\n",
    "data_directory = Path(\"WhatsApp\")\n",
    "all_chats = {}\n",
    "\n",
    "if not data_directory.exists():\n",
    "    print(\"Error: WhatsApp directory does not exist!\")\n",
    "elif not list(data_directory.glob(\"*.txt\")):\n",
    "    print(\"Error: No .txt files found in WhatsApp directory!\")\n",
    "else:\n",
    "    for file in data_directory.glob('*.txt'):\n",
    "        file_name = file.stem\n",
    "        df = read_whatsapp_chat(file)\n",
    "        if not df.empty:\n",
    "            all_chats[file_name] = df\n",
    "        else:\n",
    "            print(f\"Warning: No data extracted from {file_name}\")\n",
    "\n",
    "# Convert to Ollama training JSON format\n",
    "ollama_data = []\n",
    "\n",
    "for file_name, df in all_chats.items():\n",
    "    messages = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        role = \"assistant\" if row[\"sender\"] in ASSISTANT_NAMES else \"user\"\n",
    "        messages.append({\"role\": role, \"content\": row[\"message\"]})\n",
    "\n",
    "    if messages:\n",
    "        ollama_data.append({\n",
    "            \"system\": \"You are a helpful AI trained on WhatsApp conversations.\",\n",
    "            \"messages\": messages\n",
    "        })\n",
    "\n",
    "# Ensure output directory exists\n",
    "output_directory = Path(\"./output\")\n",
    "output_directory.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save JSON file\n",
    "output_file = output_directory / \"ollama_training.json\"\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ollama_data, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"✅ Ollama training data saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Folder containing WhatsApp chat files\n",
    "input_folder = \"WhatsApp/\"\n",
    "output_folder = \"output/\"\n",
    "\n",
    "# Ensure output folder exists\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Define assistant user\n",
    "assistant_user = \"Umyal Dixit\"\n",
    "\n",
    "# WhatsApp chat regex pattern\n",
    "pattern = r\"(\\d{2}/\\d{2}/\\d{2}), (\\d{1,2}:\\d{2} ?[ap]m) - (.*?): (.*)\"\n",
    "\n",
    "# Store all messages\n",
    "all_data = []\n",
    "\n",
    "# Process all .txt files in the folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        print(f\"🔄 Processing {filename}...\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        for line in lines:\n",
    "            match = re.match(pattern, line)\n",
    "            if match:\n",
    "                date, time, user, message = match.groups()\n",
    "                role = \"assistant\" if user == assistant_user else \"user\"\n",
    "                all_data.append({\"timestamp\": f\"{date} {time}\", \"user\": user, \"message\": message, \"role\": role})\n",
    "\n",
    "# Convert to JSONL format\n",
    "jsonl_output_file = os.path.join(output_folder, \"ollama_training.jsonl\")\n",
    "\n",
    "with open(jsonl_output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for entry in all_data:\n",
    "        json.dump(entry, f, ensure_ascii=False)\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "print(f\"✅ Training data saved to {jsonl_output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "\n",
    "# Define folder containing WhatsApp chat files\n",
    "whatsapp_folder = \"WhatsApp\"\n",
    "\n",
    "# WhatsApp message regex pattern (handles both AM/PM and 24-hour formats)\n",
    "whatsapp_pattern = r\"(\\d{2}/\\d{2}/\\d{2,4}), (\\d{1,2}:\\d{2}\\s?(?:AM|PM|am|pm)?) - ([^:]+): (.+)\"\n",
    "\n",
    "# Store extracted data\n",
    "data = []\n",
    "\n",
    "# Loop through all .txt files in the folder\n",
    "for filename in os.listdir(whatsapp_folder):\n",
    "    file_path = os.path.join(whatsapp_folder, filename)\n",
    "\n",
    "    if filename.endswith(\".txt\"):  # Process only text files\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        # Store the last user message to match with the assistant's reply\n",
    "        last_user_message = None\n",
    "        last_user_name = None\n",
    "\n",
    "        for line in lines:\n",
    "            match = re.match(whatsapp_pattern, line)\n",
    "            if match:\n",
    "                date, time, sender, message = match.groups()\n",
    "\n",
    "                # Skip system messages\n",
    "                if \"Messages and calls are end-to-end encrypted\" in message:\n",
    "                    continue\n",
    "\n",
    "                # Assign roles\n",
    "                role = \"assistant\" if sender.strip() == \"Umyal Dixit\" else \"user\"\n",
    "\n",
    "                if role == \"user\":\n",
    "                    last_user_message = message.strip()\n",
    "                    last_user_name = sender.strip()\n",
    "                elif role == \"assistant\" and last_user_message:\n",
    "                    # Create Question-Answer pair\n",
    "                    data.append({\n",
    "                        \"question\": f\"{last_user_name}: {last_user_message}\",\n",
    "                        \"answer\": f\"Umyal Dixit: {message.strip()}\"\n",
    "                    })\n",
    "                    last_user_message = None  # Reset for next pair\n",
    "\n",
    "# Convert extracted data to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Convert DataFrame to Hugging Face Dataset format\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Format for training\n",
    "dataset = dataset.map(lambda x: {\"text\": f\"### Question: {x['question']}\\n### Answer: {x['answer']}\"})\n",
    "\n",
    "# Save formatted dataset\n",
    "df.to_csv(\"whatsapp_dataset.csv\", index=False)\n",
    "dataset.to_json(\"whatsapp_dataset.json\", orient=\"records\")\n",
    "\n",
    "print(\"✅ WhatsApp chat data formatted and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig, TrainingArguments, pipeline\n",
    "from peft import LoraConfig, PeftModel\n",
    "from trl import SFTTrainer\n",
    "\n",
    "# Step 1: Load and Format WhatsApp Dataset\n",
    "df = pd.read_csv('whatsapp_dataset.csv')  # Ensure the dataset is in CSV format\n",
    "dataset = Dataset.from_pandas(df)\n",
    "dataset = dataset.map(lambda x: {\"text\": f\"### Question: {x['question']}\\n### Answer: {x['answer']}\"})\n",
    "\n",
    "# Step 2: Initialize Dolphin 3 Model & Tokenizer\n",
    "model_name = \"cognitivecomputations/dolphin-2.6-mistral\"  # Replace with Dolphin-3 when available\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# LoRA Configuration for Efficient Fine-Tuning\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# Load Base Model with Quantization (for lower VRAM usage)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=BitsAndBytesConfig(load_in_4bit=True)\n",
    ")\n",
    "\n",
    "# Step 3: Define Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./dolphin3_finetuned\",\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=2e-4,\n",
    "    fp16=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    report_to=\"none\"  # Change to \"wandb\" if using Weights & Biases\n",
    ")\n",
    "\n",
    "# Step 4: Fine-Tune the Model\n",
    "trainer = SFTTrainer(\n",
    "    model=base_model,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    "    args=training_args\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# Step 5: Save Fine-Tuned Model Locally\n",
    "trainer.model.save_pretrained('finetuned_dolphin3')\n",
    "\n",
    "# Step 6: Evaluate & Inference\n",
    "pipe = pipeline('text-generation', model=trainer.model, tokenizer=tokenizer)\n",
    "output = pipe(\"### Question: What is your name?\")[0]['generated_text']\n",
    "print(output)\n",
    "\n",
    "# Step 7: Free Up Memory\n",
    "del base_model, trainer, pipe\n",
    "gc.collect()\n",
    "\n",
    "# Step 8: Merge LoRA with Base Model & Upload to Hugging Face Hub\n",
    "base_model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16)\n",
    "model = PeftModel.from_pretrained(base_model, 'finetuned_dolphin3')\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Step 9: Push Fine-Tuned Model to Hugging Face Hub\n",
    "# model.push_to_hub(\"your_huggingface_username/finetuned_dolphin3\")\n",
    "# tokenizer.push_to_hub(\"your_huggingface_username/finetuned_dolphin3\")\n",
    "\n",
    "print(\"✅ Fine-tuned Dolphin 3 model saved & uploaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes hidden Unicode characters that may affect parsing.\"\"\"\n",
    "    return \"\".join(ch for ch in text if not unicodedata.category(ch).startswith(\"C\"))\n",
    "\n",
    "\n",
    "def read_whatsapp_chat(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a WhatsApp chat export file, processes it, and returns a structured DataFrame.\n",
    "    \"\"\"\n",
    "    # Filtering patterns to remove unwanted messages\n",
    "    encryption_message = \"Messages and calls are end-to-end encrypted\"\n",
    "    media_pattern = \"<Media omitted>\"\n",
    "    system_messages = [\n",
    "        \"created group\", \"added you\", \"removed\", \"left\", \"changed the subject\",\n",
    "        \"changed this group's icon\", \"changed the group description\",\n",
    "        \"deleted this message\", \"You deleted this message\"\n",
    "    ]\n",
    "    email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}'\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    tagging_pattern = r'@[\\w]+'\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # Clean hidden characters\n",
    "    cleaned_lines = [clean_text(line).strip() for line in lines]\n",
    "\n",
    "    # Apply filters to remove unnecessary messages\n",
    "    filtered_lines = []\n",
    "    for line in cleaned_lines:\n",
    "        if (\n",
    "            encryption_message not in line\n",
    "            and not any(msg in line for msg in system_messages)\n",
    "            and media_pattern not in line\n",
    "            and not re.search(email_pattern, line)\n",
    "            and not re.search(url_pattern, line)\n",
    "        ):\n",
    "            line = re.sub(tagging_pattern, \"\", line).strip()  # Remove @mentions\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    # Reconstruct the content for regex processing\n",
    "    content = \"\\n\".join(filtered_lines)\n",
    "\n",
    "    # WhatsApp message format regex\n",
    "    pattern = r'(\\d{1,2}/\\d{1,2}/\\d{2,4}), (\\d{1,2}:\\d{2}\\s?[APMapm]*) - (.*?): (.*)'\n",
    "\n",
    "    messages = re.findall(pattern, content)\n",
    "    if not messages:\n",
    "        print(\"❌ No messages found. Check your file format.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Convert extracted data into a DataFrame\n",
    "    df = pd.DataFrame(messages, columns=['date', 'time', 'sender', 'message'])\n",
    "\n",
    "    # Convert date and time to proper datetime format\n",
    "    df['timestamp'] = pd.to_datetime(df['date'] + \" \" + df['time'], errors='coerce')\n",
    "\n",
    "    # Drop old columns and reorder\n",
    "    df = df[['timestamp', 'sender', 'message']].dropna().sort_values(by='timestamp')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_json_format(df: pd.DataFrame, output_file: str):\n",
    "    \"\"\"\n",
    "    Converts the parsed DataFrame into the requested JSON format.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        if row['sender'].lower() == \"umyal dixit\":\n",
    "            # If Umyal Dixit is the sender, make it an output message\n",
    "            messages.append({\n",
    "                \"instruction\": df.iloc[i-1]['message'] if i > 0 else \"\",\n",
    "                \"input\": \"\",\n",
    "                \"output\": row['message']\n",
    "            })\n",
    "        else:\n",
    "            # Otherwise, it's an instruction\n",
    "            messages.append({\n",
    "                \"instruction\": row['message'],\n",
    "                \"input\": \"\",\n",
    "                \"output\": \"\"\n",
    "            })\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(messages, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ JSON file saved at: {output_file}\")\n",
    "\n",
    "\n",
    "# === Run the script ===\n",
    "input_file = \"./WhatsApp/ad.txt\"  # Replace with your actual file\n",
    "output_file = \"formatted_chat.json\"\n",
    "\n",
    "df = read_whatsapp_chat(input_file)\n",
    "\n",
    "if not df.empty:\n",
    "    convert_to_json_format(df, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"Removes hidden Unicode characters that may affect parsing.\"\"\"\n",
    "    return \"\".join(ch for ch in text if not unicodedata.category(ch).startswith(\"C\"))\n",
    "\n",
    "\n",
    "def read_whatsapp_chat(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Reads a WhatsApp chat export file, processes it, and returns a structured DataFrame.\n",
    "    \"\"\"\n",
    "    encryption_message = \"Messages and calls are end-to-end encrypted\"\n",
    "    media_pattern = \"<Media omitted>\"\n",
    "    system_messages = [\n",
    "        \"created group\", \"added you\", \"removed\", \"left\", \"changed the subject\",\n",
    "        \"changed this group's icon\", \"changed the group description\",\n",
    "        \"deleted this message\", \"You deleted this message\"\n",
    "    ]\n",
    "    email_pattern = r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}'\n",
    "    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'\n",
    "    tagging_pattern = r'@[\\w]+'\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    cleaned_lines = [clean_text(line).strip() for line in lines]\n",
    "\n",
    "    filtered_lines = []\n",
    "    for line in cleaned_lines:\n",
    "        if (\n",
    "            encryption_message not in line\n",
    "            and not any(msg in line for msg in system_messages)\n",
    "            and media_pattern not in line\n",
    "            and not re.search(email_pattern, line)\n",
    "            and not re.search(url_pattern, line)\n",
    "        ):\n",
    "            line = re.sub(tagging_pattern, \"\", line).strip()  # Remove @mentions\n",
    "            filtered_lines.append(line)\n",
    "\n",
    "    content = \"\\n\".join(filtered_lines)\n",
    "\n",
    "    pattern = r'(\\d{1,2}/\\d{1,2}/\\d{2,4}), (\\d{1,2}:\\d{2}\\s?[APMapm]*) - (.*?): (.*)'\n",
    "\n",
    "    messages = re.findall(pattern, content)\n",
    "    if not messages:\n",
    "        print(\"❌ No messages found. Check your file format.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(messages, columns=['date', 'time', 'sender', 'message'])\n",
    "    df['timestamp'] = pd.to_datetime(df['date'] + \" \" + df['time'], errors='coerce')\n",
    "    df = df[['timestamp', 'sender', 'message']].dropna().sort_values(by='timestamp')\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def convert_to_json_format(df: pd.DataFrame, output_file: str):\n",
    "    \"\"\"\n",
    "    Converts the parsed DataFrame into a structured JSON format.\n",
    "    \"\"\"\n",
    "    messages = []\n",
    "    current_instruction = []\n",
    "    current_response = []\n",
    "\n",
    "    for i, row in df.iterrows():\n",
    "        sender = row['sender'].strip()\n",
    "        message = row['message'].strip()\n",
    "\n",
    "        if sender.lower() == \"umyal dixit\":\n",
    "            if current_instruction:\n",
    "                messages.append({\n",
    "                    \"instruction\": \" \".join(current_instruction),\n",
    "                    \"input\": \"\",\n",
    "                    \"output\": \"\"\n",
    "                })\n",
    "                current_instruction = []\n",
    "\n",
    "            current_response.append(message)\n",
    "        else:\n",
    "            if current_response:\n",
    "                messages.append({\n",
    "                    \"instruction\": \"\",\n",
    "                    \"input\": \" \".join(current_instruction) if current_instruction else \"No input\",\n",
    "                    \"output\": \" \".join(current_response)\n",
    "                })\n",
    "                current_instruction = []\n",
    "                current_response = []\n",
    "\n",
    "            current_instruction.append(message)\n",
    "\n",
    "    if current_response:\n",
    "        messages.append({\n",
    "            \"instruction\": \"\",\n",
    "            \"input\": \" \".join(current_instruction) if current_instruction else \"No input\",\n",
    "            \"output\": \" \".join(current_response)\n",
    "        })\n",
    "    elif current_instruction:\n",
    "        messages.append({\n",
    "            \"instruction\": \" \".join(current_instruction),\n",
    "            \"input\": \"\",\n",
    "            \"output\": \"\"\n",
    "        })\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(messages, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"✅ JSON file saved at: {output_file}\")\n",
    "\n",
    "\n",
    "# === Run the script ===\n",
    "input_file = \"./WhatsApp/ad.txt\"  # Replace with your actual file\n",
    "output_file = \"formatted_chat.json\"\n",
    "\n",
    "df = read_whatsapp_chat(input_file)\n",
    "\n",
    "if not df.empty:\n",
    "    convert_to_json_format(df, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 25965 Q&A pairs. Saved to qa_pairs.json\n",
      "First few pairs:\n",
      "[\n",
      "  {\n",
      "    \"question\": \"Kya haal h\",\n",
      "    \"answer\": \"Tu suna\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Kya haal h\",\n",
      "    \"answer\": \"Badhiya\"\n",
      "  },\n",
      "  {\n",
      "    \"question\": \"Kkrh?\",\n",
      "    \"answer\": \"Kuch nhi\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import chardet\n",
    "from datetime import datetime\n",
    "\n",
    "def detect_and_read_file(file_path):\n",
    "    \"\"\"\n",
    "    Robustly read a file by trying multiple encoding strategies.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the file\n",
    "    \n",
    "    Returns:\n",
    "        str: Decoded file content\n",
    "    \"\"\"\n",
    "    # List of encodings to try\n",
    "    encodings_to_try = [\n",
    "        'utf-8', 'latin-1', 'cp1252', 'iso-8859-1', \n",
    "        'utf-16', 'ascii', 'big5', 'shift_jis'\n",
    "    ]\n",
    "    \n",
    "    # First, try chardet to detect encoding\n",
    "    try:\n",
    "        with open(file_path, 'rb') as file:\n",
    "            raw_data = file.read()\n",
    "            detected_encoding = chardet.detect(raw_data)['encoding']\n",
    "        \n",
    "        # Add detected encoding to the top of our try list\n",
    "        if detected_encoding:\n",
    "            encodings_to_try.insert(0, detected_encoding)\n",
    "    except Exception as e:\n",
    "        print(f\"Chardet detection failed for {file_path}: {e}\")\n",
    "    \n",
    "    # Try different encodings\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                return file.read()\n",
    "        except UnicodeDecodeError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading {file_path} with {encoding} encoding: {e}\")\n",
    "    \n",
    "    # If all else fails, try reading with error handling\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='latin-1') as file:\n",
    "            return file.read()\n",
    "    except Exception as e:\n",
    "        print(f\"Completely failed to read {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "def extract_whatsapp_messages(content):\n",
    "    \"\"\"\n",
    "    Extract WhatsApp messages from text content with enhanced robustness.\n",
    "    \n",
    "    Args:\n",
    "        content (str): Text content to parse\n",
    "    \n",
    "    Returns:\n",
    "        list: List of tuples (timestamp, sender, message)\n",
    "    \"\"\"\n",
    "    # Multiple regex patterns to catch different WhatsApp message formats\n",
    "    patterns = [\n",
    "        r'(\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2}) - (.*?): (.*?)(?=\\n\\d{2}/\\d{2}/\\d{4}, \\d{2}:\\d{2} -|$)',\n",
    "        r'(\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2}\\s*[AP]M) - (.*?): (.*?)(?=\\n\\d{1,2}/\\d{1,2}/\\d{2,4}, \\d{1,2}:\\d{2}\\s*[AP]M -|$)'\n",
    "    ]\n",
    "    \n",
    "    # Find messages using all patterns\n",
    "    all_messages = []\n",
    "    for pattern in patterns:\n",
    "        messages = re.findall(pattern, content, re.DOTALL | re.MULTILINE | re.IGNORECASE)\n",
    "        all_messages.extend(messages)\n",
    "    \n",
    "    # Filter and clean messages\n",
    "    filtered_messages = []\n",
    "    for msg in all_messages:\n",
    "        timestamp, sender, message = msg\n",
    "        \n",
    "        # Skip unwanted messages\n",
    "        unwanted_patterns = [\n",
    "            \"Messages and calls are end-to-end encrypted\",\n",
    "            \"<Media omitted>\",\n",
    "            \"created group\",\n",
    "            \"added you\",\n",
    "            \"@\",\n",
    "            re.compile(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}'),\n",
    "            re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "        ]\n",
    "        \n",
    "        # Check if message should be skipped\n",
    "        if not any(\n",
    "            (isinstance(pattern, str) and pattern in message) or \n",
    "            (isinstance(pattern, re.Pattern) and pattern.search(message))\n",
    "            for pattern in unwanted_patterns\n",
    "        ):\n",
    "            # Clean up the message\n",
    "            message = re.sub(r'<.*?>', '', message).strip()\n",
    "            \n",
    "            if message:  # Only add non-empty messages\n",
    "                filtered_messages.append((timestamp, sender, message))\n",
    "    \n",
    "    return filtered_messages\n",
    "\n",
    "def parse_timestamp(timestamp_str):\n",
    "    \"\"\"\n",
    "    Parse timestamp string with multiple format support.\n",
    "    \n",
    "    Args:\n",
    "        timestamp_str (str): Timestamp string to parse\n",
    "    \n",
    "    Returns:\n",
    "        datetime: Parsed datetime object\n",
    "    \"\"\"\n",
    "    timestamp_formats = [\n",
    "        '%d/%m/%Y, %H:%M',\n",
    "        '%m/%d/%Y, %H:%M',\n",
    "        '%d/%m/%y, %H:%M',\n",
    "        '%m/%d/%y, %H:%M',\n",
    "        '%d/%m/%Y, %I:%M %p',\n",
    "        '%m/%d/%Y, %I:%M %p',\n",
    "        '%d/%m/%y, %I:%M %p',\n",
    "        '%m/%d/%y, %I:%M %p'\n",
    "    ]\n",
    "    \n",
    "    for fmt in timestamp_formats:\n",
    "        try:\n",
    "            return datetime.strptime(timestamp_str, fmt)\n",
    "        except ValueError:\n",
    "            continue\n",
    "    \n",
    "    # If no format matches, return current time\n",
    "    print(f\"Could not parse timestamp: {timestamp_str}\")\n",
    "    return datetime.now()\n",
    "\n",
    "def process_folder_files(folder_path, answer_sender):\n",
    "    \"\"\"\n",
    "    Process all text files in a folder and extract Q&A pairs.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing text files\n",
    "        answer_sender (str): Name of the sender who provides answers\n",
    "    \n",
    "    Returns:\n",
    "        list: List of dictionaries with Q&A pairs\n",
    "    \"\"\"\n",
    "    # List all text files in the folder\n",
    "    text_files = [\n",
    "        f for f in os.listdir(folder_path) \n",
    "        if f.lower().endswith(('.txt', '.log', '.csv', '.md'))\n",
    "    ]\n",
    "    \n",
    "    # Collect all messages from all files\n",
    "    all_messages = []\n",
    "    \n",
    "    for file in text_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        # Read file content with robust encoding detection\n",
    "        content = detect_and_read_file(file_path)\n",
    "        \n",
    "        # Extract WhatsApp messages\n",
    "        file_messages = extract_whatsapp_messages(content)\n",
    "        \n",
    "        # Add file name as metadata and parse timestamp\n",
    "        for msg in file_messages:\n",
    "            timestamp, sender, message = msg\n",
    "            parsed_timestamp = parse_timestamp(timestamp)\n",
    "            all_messages.append((timestamp, parsed_timestamp, sender, message, file))\n",
    "    \n",
    "    # Convert to DataFrame for easier processing\n",
    "    df = pd.DataFrame(\n",
    "        all_messages, \n",
    "        columns=['original_timestamp', 'timestamp', 'sender', 'message', 'source_file']\n",
    "    )\n",
    "    \n",
    "    # Sort by timestamp\n",
    "    df = df.sort_values('timestamp')\n",
    "    \n",
    "    # Prepare Q&A pairs\n",
    "    qa_pairs = []\n",
    "    \n",
    "    # Get answers from specified sender\n",
    "    answers_df = df[df['sender'] == answer_sender]\n",
    "    \n",
    "    for answer in answers_df.itertuples():\n",
    "        # Find the most recent message from a different sender before this answer\n",
    "        question_candidates = df[\n",
    "            (df['timestamp'] < answer.timestamp) & \n",
    "            (df['sender'] != answer_sender)\n",
    "        ]\n",
    "        \n",
    "        if not question_candidates.empty:\n",
    "            # Take the most recent question\n",
    "            question = question_candidates.iloc[-1]\n",
    "            \n",
    "            # Only add if both question and answer are non-empty\n",
    "            if (str(question.message).strip() and \n",
    "                str(answer.message).strip()):\n",
    "                qa_pairs.append({\n",
    "                    \"question\": question.message,\n",
    "                    \"answer\": answer.message,\n",
    "                })\n",
    "    \n",
    "    return qa_pairs\n",
    "\n",
    "def main(folder_path, answer_sender, output_file='qa_pairs.json'):\n",
    "    \"\"\"\n",
    "    Main function to process files and save Q&A pairs to JSON.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (str): Path to the folder containing text files\n",
    "        answer_sender (str): Name of the sender who provides answers\n",
    "        output_file (str, optional): Path to save the output JSON file\n",
    "    \"\"\"\n",
    "    # Process chat files\n",
    "    qa_pairs = process_folder_files(folder_path, answer_sender)\n",
    "    \n",
    "    # Save to JSON\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(qa_pairs, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"Processed {len(qa_pairs)} Q&A pairs. Saved to {output_file}\")\n",
    "    print(\"First few pairs:\")\n",
    "    print(json.dumps(qa_pairs[:3], ensure_ascii=False, indent=2))\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = './WhatsApp'\n",
    "    answer_sender = 'Umyal Dixit'\n",
    "    \n",
    "    # Process and save to JSON\n",
    "    main(folder_path, answer_sender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sukkun_models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
